    1: import theano.tensor as TT
    1: import numpy as np
    1: from rllab.distributions.base import Distribution
       
       
    2: class DiagonalGaussian(Distribution):
    1:     def __init__(self, dim):
    3:         self._dim = dim
       
    1:     @property
    1:     def dim(self):
               return self._dim
       
    1:     def kl_sym(self, old_dist_info_vars, new_dist_info_vars):
    3:         old_means = old_dist_info_vars["mean"]
    3:         old_log_stds = old_dist_info_vars["log_std"]
    3:         new_means = new_dist_info_vars["mean"]
    3:         new_log_stds = new_dist_info_vars["log_std"]
               """
               Compute the KL divergence of two multivariate Gaussian distribution with
               diagonal covariance matrices
               """
    3:         old_std = TT.exp(old_log_stds)
    3:         new_std = TT.exp(new_log_stds)
               # means: (N*A)
               # std: (N*A)
               # formula:
               # { (\mu_1 - \mu_2)^2 + \sigma_1^2 - \sigma_2^2 } / (2\sigma_2^2) +
               # ln(\sigma_2/\sigma_1)
    9:         numerator = TT.square(old_means - new_means) + \
    6:                     TT.square(old_std) - TT.square(new_std)
    3:         denominator = 2 * TT.square(new_std) + 1e-8
    6:         return TT.sum(
    3:             numerator / denominator + new_log_stds - old_log_stds, axis=-1)
       
    1:     def kl(self, old_dist_info, new_dist_info):
               old_means = old_dist_info["mean"]
               old_log_stds = old_dist_info["log_std"]
               new_means = new_dist_info["mean"]
               new_log_stds = new_dist_info["log_std"]
               """
               Compute the KL divergence of two multivariate Gaussian distribution with
               diagonal covariance matrices
               """
               old_std = np.exp(old_log_stds)
               new_std = np.exp(new_log_stds)
               # means: (N*A)
               # std: (N*A)
               # formula:
               # { (\mu_1 - \mu_2)^2 + \sigma_1^2 - \sigma_2^2 } / (2\sigma_2^2) +
               # ln(\sigma_2/\sigma_1)
               numerator = np.square(old_means - new_means) + \
                           np.square(old_std) - np.square(new_std)
               denominator = 2 * np.square(new_std) + 1e-8
               return np.sum(
                   numerator / denominator + new_log_stds - old_log_stds, axis=-1)
       
    1:     def likelihood_ratio_sym(self, x_var, old_dist_info_vars, new_dist_info_vars):
    1:         logli_new = self.log_likelihood_sym(x_var, new_dist_info_vars)
    1:         logli_old = self.log_likelihood_sym(x_var, old_dist_info_vars)
    1:         return TT.exp(logli_new - logli_old)
       
    1:     def log_likelihood_sym(self, x_var, dist_info_vars):
    4:         means = dist_info_vars["mean"]
    4:         log_stds = dist_info_vars["log_std"]
    4:         zs = (x_var - means) / TT.exp(log_stds)
   12:         return - TT.sum(log_stds, axis=-1) - \
    4:                0.5 * TT.sum(TT.square(zs), axis=-1) - \
    4:                0.5 * means.shape[-1] * np.log(2 * np.pi)
       
    1:     def sample(self, dist_info):
               means = dist_info["mean"]
               log_stds = dist_info["log_std"]
               rnd = np.random.normal(size=means.shape)
               return rnd * np.exp(log_stds) + means
       
    1:     def log_likelihood(self, xs, dist_info):
 5000:         means = dist_info["mean"]
 5000:         log_stds = dist_info["log_std"]
 5000:         zs = (xs - means) / np.exp(log_stds)
15000:         return - np.sum(log_stds, axis=-1) - \
 5000:                0.5 * np.sum(np.square(zs), axis=-1) - \
 5000:                0.5 * means.shape[-1] * np.log(2 * np.pi)
       
    1:     def entropy(self, dist_info):
    2:         log_stds = dist_info["log_std"]
    2:         return np.sum(log_stds + np.log(np.sqrt(2 * np.pi * np.e)), axis=-1)
       
    1:     def entropy_sym(self, dist_info_var):
    1:         log_std_var = dist_info_var["log_std"]
    1:         return TT.sum(log_std_var + TT.log(np.sqrt(2 * np.pi * np.e)), axis=-1)
       
    1:     @property
    1:     def dist_info_keys(self):
    4:         return ["mean", "log_std"]
       
