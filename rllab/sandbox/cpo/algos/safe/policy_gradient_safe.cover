    1: import gc
    1: import numpy as np
    1: import time
    1: from rllab.algos.batch_polopt import BatchPolopt
    1: from rllab.baselines.zero_baseline import ZeroBaseline
    1: from rllab.optimizers.conjugate_gradient_optimizer import ConjugateGradientOptimizer
    1: from rllab.core.serializable import Serializable
    1: from rllab.sampler import parallel_sampler
    1: from rllab.misc.overrides import overrides
    1: from rllab.misc import ext
    1: from rllab.misc import special
    1: from rllab.misc import tensor_utils
    1: from rllab.algos import util
    1: import rllab.misc.logger as logger
    1: import theano
    1: import theano.tensor as TT
       
       
    1: from sandbox.cpo.algos.safe.sampler_safe import BatchSamplerSafe
       
    2: class PolicyGradientSafe(BatchPolopt, Serializable):
    1:     """
           Policy Gradient base algorithm
       
           with optional data reuse and importance sampling,
           and exploration bonuses
       
           also with safety constraints
       
           Can use this as a base class for VPG, ERWR, TNPG, TRPO, etc. by picking appropriate optimizers / arguments
       
           for VPG: use FirstOrderOptimizer
           for ERWR: set positive_adv to True
           for TNPG: use ConjugateGradient optimizer with max_backtracks=1
           for TRPO: use ConjugateGradient optimizer with max_backtracks>1
           for PPO: use PenaltyLBFGS optimzer
       
           """
       
    1:     def __init__(
                   self,
                   neptune_instance,
                   optimizer=None,
                   optimizer_args=None,
                   all_paths=True,
                   step_size=0.01,
                   safety_constrained_optimizer=True,
                   safety_constraint=None,
                   safety_key='advantages',
                   safety_discount=1,
                   safety_gae_lambda=1,
                   center_safety_vals=True,
                   robustness_coeff=0.,
                   attempt_feasible_recovery=True,
                   attempt_infeasible_recovery=True,
                   revert_to_last_safe_point=False,
                   safety_tradeoff=False,
                   safety_tradeoff_coeff=0,
                   learn_safety_tradeoff_coeff=False,
                   safety_tradeoff_coeff_lr=1e-2,
                   pdo_vf_mode=1,
                   entropy_regularize=False,
                   entropy_coeff=1e-4,
                   entropy_coeff_decay=1,
                   exploration_bonus=None,
                   exploration_lambda=0.001,
                   normalize_bonus=True,
                   nonnegative_bonus_mean=False,
                   batch_aggregate_n=1,
                   batch_aggregate_coeff=0.5,
                   relative_weights=False,
                   importance_sampling=False,
                   decision_weight_mode='pd',
                   clip_IS_coeff_above=False,
                   clip_IS_coeff_below=False,
                   IS_coeff_upper_bound=5,
                   IS_coeff_lower_bound=0,
                   **kwargs):
       
       
               """
               :param batch_aggregate_n: use this many epochs of data (including current)
               :param batch_aggregate_coeff: used to make contribution of old data smaller. formula:
       
                   If a batch has age j, it is weighted proportionally to
       
                                                 batch_aggregate_coeff ** j,
       
                   with these batch weights normalized.
       
                   If you want every batch to have equal weight, set batch_aggregate_coeff = 1. 
       
               :param relative_weights: used to make contribution of old data invariant to how many
                                        more or fewer trajectories the old batch may have.
               :param importance_sampling: do or do not use importance sampling to reweight old data
               :param clip_IS_coeff: if true, clip the IS coefficients.
               :param IS_coeff_bound: if clip_IS_coeff, then IS coefficients are bounded by this value. 
               :param decision_weight_mode: either 'pd', per decision, or 'pt', per trajectory
       
               """
       
    1:         Serializable.quick_init(self, locals())
    1:         self.neptune_instance = neptune_instance
    1:         self.optimizer = optimizer
    1:         self.all_paths = all_paths
       
               # npo
    1:         self.step_size = step_size
       
               # safety
    1:         self.safety_constrained_optimizer = safety_constrained_optimizer
    1:         self.safety_constraint = safety_constraint
    1:         self.safety_step_size = self.safety_constraint.get_safety_step()
    1:         assert(safety_key in ['rewards','returns','advantages'])
               #destul de clar aici, nu trebuie sa schimb nimic
    1:         if safety_key == 'advantages' and not(hasattr(self.safety_constraint,'baseline')):
                   logger.log("Warning: selected advantages as safety key without providing baseline.")
                   logger.log("Falling back on returns as safety key.")
                   safety_key = 'returns'
    1:         self.safety_key = 'safety_'+safety_key
    1:         self.safety_discount = safety_discount
    1:         self.safety_gae_lambda = safety_gae_lambda
    1:         self.center_safety_vals = center_safety_vals
    1:         self.robustness_coeff = robustness_coeff
    1:         self.attempt_feasible_recovery=attempt_feasible_recovery
    1:         self.attempt_infeasible_recovery=attempt_infeasible_recovery
    1:         self.revert_to_last_safe_point=revert_to_last_safe_point
       
               # safety tradeoff
    1:         self.safety_tradeoff = safety_tradeoff
    1:         self.safety_tradeoff_coeff = 1. * safety_tradeoff_coeff 
    1:         self.learn_safety_tradeoff_coeff = learn_safety_tradeoff_coeff
    1:         self.safety_tradeoff_coeff_lr = safety_tradeoff_coeff_lr
    1:         self.pdo_vf_mode = pdo_vf_mode      #1 = one VF for R + alpha*S 
                                                   #2 = two VFs (one for R, one for S)
                                                   #Experiments in the paper use mode 1,
                                                   #although I tried out both. 
                                                   #(Mode 2 seemed less stable.)
       
               # entropy regularization
    1:         self.entropy_regularize = entropy_regularize
    1:         self.entropy_coeff = entropy_coeff
    1:         self.entropy_coeff_decay = entropy_coeff_decay
       
               # intrinsic motivation
    1:         self.exploration_bonus = exploration_bonus
    1:         self.exploration_lambda = exploration_lambda
    1:         self.normalize_bonus = normalize_bonus
    1:         self.nonnegative_bonus_mean = nonnegative_bonus_mean
       
               # importance sampling
    1:         self.importance_sampling = importance_sampling
    1:         self.decision_weight_mode = decision_weight_mode
    1:         self.clip_IS_coeff_above = clip_IS_coeff_above
    1:         self.clip_IS_coeff_below = clip_IS_coeff_below
    1:         self.IS_coeff_upper_bound = IS_coeff_upper_bound
    1:         self.IS_coeff_lower_bound = IS_coeff_lower_bound
    1:         self.batch_aggregate_n = batch_aggregate_n
    1:         self.batch_aggregate_coeff = batch_aggregate_coeff
    1:         self.relative_weights = relative_weights
       
    3:         super(PolicyGradientSafe, self).__init__(optimizer=optimizer, 
    1:                                                  neptune_instance=neptune_instance,
    1:                                                  sampler_cls=BatchSamplerSafe,
    1:                                                  **kwargs)
       
               # safety tradeoff
    1:         if self.safety_constraint and self.safety_tradeoff and self.pdo_vf_mode == 1:
                   self.baseline._target_key = 'tradeoff_returns'
               
       
    1:     @overrides
    1:     def init_opt(self):
    1:         self.start_time = time.time()
    1:         is_recurrent = int(self.policy.recurrent)
    2:         obs_var = self.env.observation_space.new_tensor_variable(
    1:             'obs',
    1:             extra_dims=1 + is_recurrent,
               )
    2:         action_var = self.env.action_space.new_tensor_variable(
    1:             'action',
    1:             extra_dims=1 + is_recurrent,
               )
    2:         advantage_var = ext.new_tensor(
    1:             'advantage',
    1:             ndim=1 + is_recurrent,
    1:             dtype=theano.config.floatX
               )
    1:         if self.safety_constraint:
    2:             safety_var = ext.new_tensor(
    1:                 'safety_vals',
    1:                 ndim=1 + is_recurrent,
    1:                 dtype=theano.config.floatX
                   )
                   
       
    2:         weights_var = ext.new_tensor(
    1:             'weights',
    1:             ndim=1 + is_recurrent,
    1:             dtype=theano.config.floatX
               )
    1:         dist = self.policy.distribution
    5:         old_dist_info_vars = {
    4:             k: ext.new_tensor(
    2:                 'old_%s' % k,
    2:                 ndim=2 + is_recurrent,
    2:                 dtype=theano.config.floatX
    3:             ) for k in dist.dist_info_keys
                   }
    4:         old_dist_info_vars_list = [old_dist_info_vars[k] for k in dist.dist_info_keys]
       
    3:         state_info_vars = {
                   k: ext.new_tensor(
                       k,
                       ndim=2 + is_recurrent,
                       dtype=theano.config.floatX
    1:             ) for k in self.policy.state_info_keys
               }
    2:         state_info_vars_list = [state_info_vars[k] for k in self.policy.state_info_keys]
       
    1:         if is_recurrent:
                   valid_var = TT.matrix('valid')
               else:
    1:             valid_var = None
       
    1:         dist_info_vars = self.policy.dist_info_sym(obs_var, state_info_vars)
       
    2:         self.dist_info_vars_func = ext.compile_function(
    1:                 inputs=[obs_var] + state_info_vars_list,
    1:                 outputs=dist_info_vars,
    1:                 log_name="dist_info_vars"
                   )
       
               # when we want to get D_KL( pi' || pi) for data that was sampled on 
               # some behavior policy pi_b, where pi' is the optimization variable
               # and pi is the policy of the previous iteration,
               # the dist_info in memory will correspond to pi_b and not pi. 
               # so we have to compute the dist_info for that data on pi, on the fly.
       
    1:         ent = dist.entropy_sym(dist_info_vars)
    1:         kl = dist.kl_sym(old_dist_info_vars, dist_info_vars)
    1:         lr = dist.likelihood_ratio_sym(action_var, old_dist_info_vars, dist_info_vars)
    1:         if is_recurrent:
                   mean_ent = TT.sum(weights_var * ent * valid_var) / TT.sum(valid_var)
                   max_kl = TT.max(kl * valid_var)
                   mean_kl = TT.sum(weights_var * kl * valid_var) / TT.sum(valid_var)
                   surr_loss = - TT.sum(lr * weights_var * advantage_var * valid_var) / TT.sum(valid_var)
                   if self.safety_constraint:
                       f_safety = TT.sum(lr * weights_var * safety_var * valid_var) / TT.sum(valid_var)
               else:
    1:             mean_ent = TT.mean(weights_var * ent)
    1:             max_kl = TT.max(kl)
    1:             mean_kl = TT.mean(weights_var * kl)
    1:             surr_loss = - TT.mean(lr * weights_var * advantage_var)
    1:             if self.safety_constraint:
    1:                 f_safety = TT.mean(lr * weights_var * safety_var)
       
    1:         if self.entropy_regularize:
                   self.entropy_beta = theano.shared(self.entropy_coeff)
                   surr_loss -= self.entropy_beta * mean_ent
       
    1:         if self.safety_constraint:
    1:             self.safety_gradient_rescale = theano.shared(1.)
    1:             f_safety = self.safety_gradient_rescale * f_safety
       
       
    1:         input_list = [
    1:                          obs_var,
    1:                          action_var,
    1:                          advantage_var,
    1:                          weights_var,
                            ]
       
    1:         if self.safety_constraint:
    1:             input_list.append(safety_var)
       
    1:         input_list = input_list + state_info_vars_list + old_dist_info_vars_list
    1:         if is_recurrent:
                   input_list.append(valid_var)
       
       
    1:         if not(self.safety_constrained_optimizer):
                   self.optimizer.update_opt(
                       loss=surr_loss,
                       target=self.policy,
                       leq_constraint=(mean_kl, self.step_size),
                       inputs=input_list,
                       constraint_name="mean_kl"
                   )
               else:
    2:             self.optimizer.update_opt(
    1:                 loss=surr_loss,
    1:                 target=self.policy,
    1:                 quad_leq_constraint=(mean_kl, self.step_size),
    1:                 lin_leq_constraint=(f_safety, self.safety_step_size),
    1:                 inputs=input_list,
    1:                 constraint_name_1="mean_kl",
    1:                 constraint_name_2="safety",
    1:                 using_surrogate=False,
    1:                 precompute=True,
    1:                 attempt_feasible_recovery=self.attempt_feasible_recovery,
    1:                 attempt_infeasible_recovery=self.attempt_infeasible_recovery,
    1:                 revert_to_last_safe_point=self.revert_to_last_safe_point
                   )
       
       
    2:         f_kl = ext.compile_function(
    1:             inputs=input_list,
    1:             outputs=[mean_kl, max_kl],
               )
    2:         self.opt_info = dict(
    1:             f_kl=f_kl,
               )
       
       
       
    1:     @overrides
    1:     def optimize_policy(self, itr, samples_data):
    2:         logger.log('optimizing policy...')
    4:         all_input_values = tuple(ext.extract(
    2:             samples_data,
    2:             "observations", "actions", "advantages", "weights"
                   ))
    2:         if self.safety_constraint:
    2:             all_input_values += tuple(ext.extract(samples_data,"safety_values"))
    2:             self.safety_gradient_rescale.set_value(samples_data['safety_rescale'])
    2:             logger.record_tabular('SafetyGradientRescale', self.safety_gradient_rescale.get_value())
                   """
                   I think this one is worth some explanation. The surrogate function is computed by taking
                   an average of likelihood ratios times safety advantages. IE, it is a sample expectation 
                   over state-action pairs. Suppose we have N trajectories of length T. Then the surrogate is
       
                       surrogate = (1 / NT) * sum_{j=1}^N sum_{t=1}^T lr(j,t) * adv(j,t)
       
                   But the true safety constraint function is an expectation over /trajectories/, not state-action
                   pairs. 
       
                       true constraint = (1 / N) * sum_{j=1}^N sum_{t=1}^T lr(j,t) * adv(j,t)
                                       = T * surrogate
       
                   So the gradient of the surrogate is (1 / T) times the gradient of the true constraint. 
                   In normal policy gradient situations, this isn't a problem, because we only care about the
                   direction and not the magnitude. But, our safety constraint formulation crucially relies
                   on this gradient having the correct magnitude! So we have to rescale appropriately. 
                   The "SafetyGradientRescale" is automatically computed by the sampler and provided to 
                   the optimizer.
                   """
       
    2:         agent_infos = samples_data["agent_infos"]
    4:         state_info_list = [agent_infos[k] for k in self.policy.state_info_keys]
    8:         dist_info_list = [agent_infos[k] for k in self.policy.distribution.dist_info_keys]
    2:         all_input_values += tuple(state_info_list) + tuple(dist_info_list)
    2:         if self.policy.recurrent:
                   all_input_values += (samples_data["valids"],)
    2:         loss_before = self.optimizer.loss(all_input_values)
       
       
    2:         if not(self.safety_constrained_optimizer):
                   self.optimizer.optimize(all_input_values)
               else:
    2:             threshold = max(self.safety_step_size - samples_data['safety_eval'],0)
    2:             if 'advantage' in self.safety_key:
    2:                 std_adv = np.std(samples_data["safety_values"])
    2:                 logger.record_tabular('StdSafetyAdv',std_adv)
    2:                 threshold = max(threshold - self.robustness_coeff*std_adv,0)
                   
    2:             if 'safety_offset' in samples_data:
    2:                 logger.record_tabular('SafetyOffset',samples_data['safety_offset'])
       
    4:             self.optimizer.optimize(all_input_values,
    2:                     precomputed_eval = samples_data['safety_eval'],
    2:                     precomputed_threshold = threshold,
    2:                     diff_threshold=True)
       
    2:         mean_kl, max_kl = self.opt_info['f_kl'](*all_input_values)
    2:         loss_after = self.optimizer.loss(all_input_values)
       
    2:         if self.entropy_regularize and not(self.entropy_coeff_decay == 1):
                   current_entropy_coeff = self.entropy_beta.get_value() * self.entropy_coeff_decay
                   self.entropy_beta.set_value(current_entropy_coeff)
                   logger.record_tabular('EntropyCoeff', current_entropy_coeff)
       
       
    2:         if self.learn_safety_tradeoff_coeff:
                   delta = samples_data['safety_eval'] - self.safety_step_size
                   logger.record_tabular('TradeoffCoeffBefore',self.safety_tradeoff_coeff)
                   self.safety_tradeoff_coeff += self.safety_tradeoff_coeff_lr * delta
                   self.safety_tradeoff_coeff = max(0, self.safety_tradeoff_coeff)
                   logger.record_tabular('TradeoffCoeffAfter',self.safety_tradeoff_coeff)
                   
    2:         logger.record_tabular('Time',time.time() - self.start_time)
    2:         logger.record_tabular('LossBefore', loss_before)
    2:         logger.record_tabular('LossAfter', loss_after)
    2:         logger.record_tabular('MeanKL', mean_kl)
    2:         logger.record_tabular('MaxKL', max_kl)
    2:         logger.record_tabular('dLoss', loss_before - loss_after)
    2:         logger.log('optimization finished')
       
       
    1:     @overrides
    1:     def get_itr_snapshot(self, itr, samples_data):
    4:         return dict(
    2:             itr=itr,
    2:             policy=self.policy,
    2:             baseline=self.baseline,
    2:             env=self.env,
    2:             expl=self.exploration_bonus,
    2:             safe=self.safety_constraint
               )
