    1: from rllab.misc import ext
    1: from rllab.misc import krylov
    1: from rllab.misc import logger
    1: from rllab.core.serializable import Serializable
    1: import theano.tensor as TT
    1: import theano
    1: import itertools
    1: import numpy as np
    1: from rllab.misc.ext import sliced_fun
       # from _ast import Num
       
       
    2: class PerlmutterHvp(Serializable):
       
    1:     def __init__(self, num_slices=1):
    2:         Serializable.quick_init(self, locals())
    2:         self.target = None
    2:         self.reg_coeff = None
    2:         self.opt_fun = None
    2:         self._num_slices = num_slices
       
    1:     def update_opt(self, f, target, inputs, reg_coeff):
    2:         self.target = target
    2:         self.reg_coeff = reg_coeff
    2:         params = target.get_params(trainable=True)
       
    4:         constraint_grads = theano.grad(
    2:             f, wrt=params, disconnected_inputs='warn')
   16:         xs = tuple([ext.new_tensor_like("%s x" % p.name, p) for p in params])
       
    2:         def Hx_plain():
    4:             Hx_plain_splits = TT.grad(
   30:                 TT.sum([TT.sum(g * x)
   14:                         for g, x in zip(constraint_grads, xs)]),
    2:                 wrt=params,
    2:                 disconnected_inputs='warn'
                   )
   16:             return TT.concatenate([TT.flatten(s) for s in Hx_plain_splits])
       
    4:         self.opt_fun = ext.lazydict(
    6:             f_Hx_plain=lambda: ext.compile_function(
    2:                 inputs=inputs + xs,
    2:                 outputs=Hx_plain(),
    2:                 log_name="f_Hx_plain",
                   ),
               )
       
    1:     def build_eval(self, inputs):
    4:         def eval(x):
   48:             xs = tuple(self.target.flat_to_params(x, trainable=True))
  144:             ret = sliced_fun(self.opt_fun["f_Hx_plain"], self._num_slices)(
   96:                 inputs, xs) + self.reg_coeff * x
   48:             return ret
       
    4:         return eval
       
       
    2: class FiniteDifferenceHvp(Serializable):
       
    1:     def __init__(self, base_eps=1e-8, symmetric=True, grad_clip=None, num_slices=1):
               Serializable.quick_init(self, locals())
               self.base_eps = base_eps
               self.symmetric = symmetric
               self.grad_clip = grad_clip
               self._num_slices = num_slices
       
    1:     def update_opt(self, f, target, inputs, reg_coeff):
               self.target = target
               self.reg_coeff = reg_coeff
       
               params = target.get_params(trainable=True)
       
               constraint_grads = theano.grad(
                   f, wrt=params, disconnected_inputs='warn')
               flat_grad = ext.flatten_tensor_variables(constraint_grads)
       
               def f_Hx_plain(*args):
                   inputs_ = args[:len(inputs)]
                   xs = args[len(inputs):]
                   flat_xs = np.concatenate([np.reshape(x, (-1,)) for x in xs])
                   param_val = self.target.get_param_values(trainable=True)
                   eps = np.cast['float32'](
                       self.base_eps / (np.linalg.norm(param_val) + 1e-8))
                   self.target.set_param_values(
                       param_val + eps * flat_xs, trainable=True)
                   flat_grad_dvplus = self.opt_fun["f_grad"](*inputs_)
                   if self.symmetric:
                       self.target.set_param_values(
                           param_val - eps * flat_xs, trainable=True)
                       flat_grad_dvminus = self.opt_fun["f_grad"](*inputs_)
                       hx = (flat_grad_dvplus - flat_grad_dvminus) / (2 * eps)
                       self.target.set_param_values(param_val, trainable=True)
                   else:
                       self.target.set_param_values(param_val, trainable=True)
                       flat_grad = self.opt_fun["f_grad"](*inputs_)
                       hx = (flat_grad_dvplus - flat_grad) / eps
                   return hx
       
               self.opt_fun = ext.lazydict(
                   f_grad=lambda: ext.compile_function(
                       inputs=inputs,
                       outputs=flat_grad,
                       log_name="f_grad",
                   ),
                   f_Hx_plain=lambda: f_Hx_plain,
               )
       
    1:     def build_eval(self, inputs):
               def eval(x):
                   xs = tuple(self.target.flat_to_params(x, trainable=True))
                   ret = sliced_fun(self.opt_fun["f_Hx_plain"], self._num_slices)(
                       inputs, xs) + self.reg_coeff * x
                   return ret
       
               return eval
       
       
    2: class ConjugateGradientOptimizer(Serializable):
    1:     """
           Performs constrained optimization via line search. The search direction is computed using a conjugate gradient
           algorithm, which gives x = A^{-1}g, where A is a second order approximation of the constraint and g is the gradient
           of the loss function.
           """
       
    1:     def __init__(
                   self,
                   cg_iters=10,
                   reg_coeff=1e-5,
                   subsample_factor=1.,
                   backtrack_ratio=0.8,
                   max_backtracks=15,
                   accept_violation=False,
                   hvp_approach=None,
                   num_slices=1):
               """
       
               :param cg_iters: The number of CG iterations used to calculate A^-1 g
               :param reg_coeff: A small value so that A -> A + reg*I
               :param subsample_factor: Subsampling factor to reduce samples when using "conjugate gradient. Since the
               computation time for the descent direction dominates, this can greatly reduce the overall computation time.
               :param accept_violation: whether to accept the descent step if it violates the line search condition after
               exhausting all backtracking budgets
               :return:
               """
    2:         Serializable.quick_init(self, locals())
    2:         self._cg_iters = cg_iters
    2:         self._reg_coeff = reg_coeff
    2:         self._subsample_factor = subsample_factor
    2:         self._backtrack_ratio = backtrack_ratio
    2:         self._max_backtracks = max_backtracks
    2:         self._num_slices = num_slices
       
    2:         self._opt_fun = None
    2:         self._target = None
    2:         self._max_constraint_val = None
    2:         self._constraint_name = None
    2:         self._accept_violation = accept_violation
    2:         if hvp_approach is None:
    2:             hvp_approach = PerlmutterHvp(num_slices)
    2:         self._hvp_approach = hvp_approach
       
    1:     def update_opt(self, loss, target, leq_constraint, inputs, extra_inputs=None, constraint_name="constraint", *args,
                          **kwargs):
               """
               :param loss: Symbolic expression for the loss function.
               :param target: A parameterized object to optimize over. It should implement methods of the
               :class:`rllab.core.paramerized.Parameterized` class.
               :param leq_constraint: A constraint provided as a tuple (f, epsilon), of the form f(*inputs) <= epsilon.
               :param inputs: A list of symbolic variables as inputs, which could be subsampled if needed. It is assumed
               that the first dimension of these inputs should correspond to the number of data points
               :param extra_inputs: A list of symbolic variables as extra inputs which should not be subsampled
               :return: No return value.
               """
       
    2:         inputs = tuple(inputs)
    2:         if extra_inputs is None:
    2:             extra_inputs = tuple()
               else:
                   extra_inputs = tuple(extra_inputs)
       
    2:         constraint_term, constraint_value = leq_constraint
       
    2:         params = target.get_params(trainable=True)
    2:         grads = theano.grad(loss, wrt=params, disconnected_inputs='warn')
    2:         flat_grad = ext.flatten_tensor_variables(grads)
       
    4:         self._hvp_approach.update_opt(f=constraint_term, target=target, inputs=inputs + extra_inputs,
    2:                                       reg_coeff=self._reg_coeff)
       
    2:         self._target = target
    2:         self._max_constraint_val = constraint_value
    2:         self._constraint_name = constraint_name
       
    4:         self._opt_fun = ext.lazydict(
    6:             f_loss=lambda: ext.compile_function(
    2:                 inputs=inputs + extra_inputs,
    2:                 outputs=loss,
    2:                 log_name="f_loss",
                   ),
    6:             f_grad=lambda: ext.compile_function(
    2:                 inputs=inputs + extra_inputs,
    2:                 outputs=flat_grad,
    2:                 log_name="f_grad",
                   ),
    6:             f_constraint=lambda: ext.compile_function(
    2:                 inputs=inputs + extra_inputs,
    2:                 outputs=constraint_term,
    2:                 log_name="constraint",
                   ),
    6:             f_loss_constraint=lambda: ext.compile_function(
    2:                 inputs=inputs + extra_inputs,
    2:                 outputs=[loss, constraint_term],
    2:                 log_name="f_loss_constraint",
                   ),
               )
       
    1:     def loss(self, inputs, extra_inputs=None):
    8:         inputs = tuple(inputs)
    8:         if extra_inputs is None:
    8:             extra_inputs = tuple()
    8:         return sliced_fun(self._opt_fun["f_loss"], self._num_slices)(inputs, extra_inputs)
       
    1:     def constraint_val(self, inputs, extra_inputs=None):
    4:         inputs = tuple(inputs)
    4:         if extra_inputs is None:
    4:             extra_inputs = tuple()
    4:         return sliced_fun(self._opt_fun["f_constraint"], self._num_slices)(inputs, extra_inputs)
       
    1:     def optimize(self, inputs, extra_inputs=None, subsample_grouped_inputs=None):
       
    4:         inputs = tuple(inputs)
    4:         if extra_inputs is None:
    4:             extra_inputs = tuple()
       
    4:         if self._subsample_factor < 1:
    4:             if subsample_grouped_inputs is None:
    4:                 subsample_grouped_inputs = [inputs]
    4:             subsample_inputs = tuple()
    8:             for inputs_grouped in subsample_grouped_inputs:
    4:                 n_samples = len(inputs_grouped[0])
    8:                 inds = np.random.choice(
    4:                     n_samples, int(n_samples * self._subsample_factor), replace=False)
   24:                 subsample_inputs += tuple([x[inds] for x in inputs_grouped])
               else:
                   subsample_inputs = inputs
       
    4:         logger.log("computing loss before")
    8:         loss_before = sliced_fun(self._opt_fun["f_loss"], self._num_slices)(
    4:             inputs, extra_inputs)
    4:         logger.log("performing update")
    4:         logger.log("computing descent direction")
       
    8:         flat_g = sliced_fun(self._opt_fun["f_grad"], self._num_slices)(
    4:             inputs, extra_inputs)
       
    4:         Hx = self._hvp_approach.build_eval(subsample_inputs + extra_inputs)
       
    4:         descent_direction = krylov.cg(Hx, flat_g, cg_iters=self._cg_iters)
       
    4:         approx_g = Hx(descent_direction)
    4:         q = descent_direction.dot(approx_g)
    4:         residual = np.sqrt((approx_g - flat_g).dot(approx_g - flat_g))
    4:         rescale  = q / (descent_direction.dot(descent_direction))
    4:         logger.record_tabular("OptimDiagnostic_Residual",residual)
    4:         logger.record_tabular("OptimDiagnostic_Rescale", rescale)
       
    8:         initial_step_size = np.sqrt(
    8:             2.0 * self._max_constraint_val *
    4:             (1. / (descent_direction.dot(Hx(descent_direction)) + 1e-8))
               )
    4:         if np.isnan(initial_step_size):
                   initial_step_size = 1.
    4:         flat_descent_step = initial_step_size * descent_direction
       
    4:         logger.log("descent direction computed")
       
    4:         prev_param = np.copy(self._target.get_param_values(trainable=True))
    4:         n_iter = 0
    4:         for n_iter, ratio in enumerate(self._backtrack_ratio ** np.arange(self._max_backtracks)):
    4:             cur_step = ratio * flat_descent_step
    4:             cur_param = prev_param - cur_step
    4:             self._target.set_param_values(cur_param, trainable=True)
   12:             loss, constraint_val = sliced_fun(
    8:                 self._opt_fun["f_loss_constraint"], self._num_slices)(inputs, extra_inputs)
    4:             if loss < loss_before and constraint_val <= self._max_constraint_val:
    4:                 break
    8:         if (np.isnan(loss) or np.isnan(constraint_val) or loss >= loss_before or constraint_val >=
    4:                 self._max_constraint_val) and not self._accept_violation:
                   logger.log("Line search condition violated. Rejecting the step!")
                   if np.isnan(loss):
                       logger.log("Violated because loss is NaN")
                   if np.isnan(constraint_val):
                       logger.log("Violated because constraint %s is NaN" %
                                  self._constraint_name)
                   if loss >= loss_before:
                       logger.log("Violated because loss not improving")
                   if constraint_val >= self._max_constraint_val:
                       logger.log(
                           "Violated because constraint %s is violated" % self._constraint_name)
                   self._target.set_param_values(prev_param, trainable=True)
    4:         logger.log("backtrack iters: %d" % n_iter)
    4:         logger.log("computing loss after")
    4:         logger.log("optimization finished")
